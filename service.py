# demo.py
from modules.chatbot_qwen import QwenChatEngine

# 1ï¸âƒ£ å»ºç«‹å¼•æ“ï¼ˆåªè¦æä¾›æ¨¡å‹è·¯å¾‘å³å¯ï¼‰
chat_engine = QwenChatEngine(
    model_path="models/Qwen3-8B_int4",
    device="cuda",          # singleâ€‘GPU
    dtype="auto",
    quantization="bitsandbytes",   # 4â€‘bit via bitsandbytes
    trust_remote_code=True,
    max_model_len=7936,    # è‹¥æƒ³æ”¯æ´è¶…é•·ä¸Šä¸‹æ–‡
    first_trim_k=5,
)

# 2ï¸âƒ£ è¼‰å…¥æ¨¡å‹èˆ‡ tokenizer
chat_engine.load()

# 3ï¸âƒ£ é–‹å§‹æœƒè©±ï¼ˆå¯åŠ å…¥ system promptï¼‰
chat_engine.start_conv(system_prompt="You are a helpful assistant.")

exit_conv = False 
while exit_conv == False:
    usr_input = input("User: ")
    if usr_input == 'exit':
        exit_conv = True
        break
    else:
        resp = chat_engine.chat(
            usr_input,
            enable_thinking=True,
            max_new_tokens=3840,
            temperature=0.55,
            top_p=0.9,
        )
        print("ğŸ§  Thinking:\n", resp["thinking"])
        print("ğŸ¤– Answer:\n", resp["answer"])




# 6ï¸âƒ£ çµæŸæœƒè©±ã€é‡‹æ”¾è³‡æº
chat_engine.end_conv()
# è‹¥ç¨‹å¼çµæŸï¼Œ__del__ æœƒè‡ªå‹•å‘¼å« vLLM shutdown

