# demo.py
from modules.chatbot import QwenChatEngine

# 1ï¸âƒ£ å»ºç«‹å¼•æ“ï¼ˆåªè¦æä¾›æ¨¡å‹è·¯å¾‘å³å¯ï¼‰
engine = QwenChatEngine(
    model_path="models/Qwen3-8B_int4",
    device="cuda",          # singleâ€‘GPU
    dtype="auto",
    quantization="bitsandbytes",   # 4â€‘bit via bitsandbytes
    trust_remote_code=True,
    max_model_len=7936,    # è‹¥æƒ³æ”¯æ´è¶…é•·ä¸Šä¸‹æ–‡
    first_trim_k=5,
)

# 2ï¸âƒ£ è¼‰å…¥æ¨¡å‹èˆ‡ tokenizer
engine.load()

# 3ï¸âƒ£ é–‹å§‹æœƒè©±ï¼ˆå¯åŠ å…¥ system promptï¼‰
engine.start_conv(system_prompt="You are a helpful assistant.")

exit_conv = False 
# 4ï¸âƒ£ ç¬¬ä¸€æ¬¡å°è©± â†’ é–‹å•Ÿæ€è€ƒæ¨¡å¼
while exit_conv == False:
    usr_input = input("User: ")
    if usr_input == 'exit':
        exit_conv = True
        continue
    resp = engine.chat(
        usr_input,
        enable_thinking=True,
        max_new_tokens=2048,
        temperature=0.6,
        top_p=0.95,
    )
    print("ğŸ§  Thinking:\n", resp["thinking"])
    print("ğŸ¤– Answer:\n", resp["answer"])


# 6ï¸âƒ£ çµæŸæœƒè©±ã€é‡‹æ”¾è³‡æº
engine.end_conv()
# è‹¥ç¨‹å¼çµæŸï¼Œ__del__ æœƒè‡ªå‹•å‘¼å« vLLM shutdown
